{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "from torchtext import vocab, data\n",
    "from fastai.learner import *\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process dataset - create dataset with \n",
    "\n",
    "PATH='/home/ubuntu/course-shortcut/competitions/lyrics_generator/'\n",
    "TRN = \"/home/ubuntu/course-shortcut/competitions/lyrics_generator/all/trn/\"\n",
    "VAL = \"/home/ubuntu/course-shortcut/competitions/lyrics_generator/all/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paths = glob(f'{TRN}/*.txt')\n",
    "    \n",
    "\n",
    "# text_field = data.Field(lower=True, tokenize=my_spacy_tok)\n",
    "my_tok = spacy.load('en')\n",
    "def my_spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]\n",
    "\n",
    "text = []\n",
    "text_field = data.Field(lower=True, tokenize=my_spacy_tok)\n",
    "for p in paths:\n",
    "    for line in open(p): text += text_field.preprocess(line)\n",
    "    text.append('<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paths = glob(f'{VAL}/*.txt')\n",
    "\n",
    "# text_field = data.Field(lower=True, tokenize=my_spacy_tok)\n",
    "my_tok = spacy.load('en')\n",
    "def my_spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]\n",
    "\n",
    "val_text = []\n",
    "text_field = data.Field(lower=True, tokenize=my_spacy_tok)\n",
    "for p in paths:\n",
    "    for line in open(p): val_text += text_field.preprocess(line)\n",
    "    val_text.append('<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Save the word list\n",
    "trn_tokens = '/home/ubuntu/tokenized_words.pkl'\n",
    "val_tokens = '/home/ubuntu/val_tokenized_words.pkl'\n",
    "# pickle.dump(val_text, open('/home/ubuntu/val_tokenized_words.pkl','wb'))\n",
    "# pickle.dump(text, open('/home/ubuntu/tokenized_words.pkl','wb'))\n",
    "my_tok = spacy.load('en')\n",
    "def my_spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]\n",
    "text_field = data.Field(lower=True, tokenize=my_spacy_tok)\n",
    "\n",
    "text = pickle.load(open(trn_tokens, 'rb'))\n",
    "val_text = pickle.load(open(val_tokens, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file.writelines\n",
    "# file = open(\"/home/ubuntu/course-shortcut/competitions/lyrics_generator/all/trn.txt\", 'w')\n",
    "\n",
    "# for p in paths:\n",
    "#     file.write(open(p).read())\n",
    "#     file.write('<eos>')\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet(text_field, text):\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [data.Example.fromlist([text], fields)]\n",
    "    return data.Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = createDataSet(text_field, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = createDataSet(text_field, val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(trn_ds, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16709"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', ',', 'i', 'the', 'you', 'a', 'it', 'and', 'to', 'my', 'me']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 500",
    "nh = 500",
    "nl = 3",
    "bs=32\n",
    "bptt=300 #backpropogate through time: number of words it'll remember\n",
    "\n",
    "FILES = dict(train=TRN, validation=VAL, test=VAL)\n",
    "\n",
    "md = LanguageModelData.from_text_files(PATH, text_field, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_function = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "learner = md.get_model(optimization_function, em_sz, nh, nl,\n",
    "dropout=0.3, dropouth=0.3, dropouti=0.3, dropoute=0.3, wdrop=0.2)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52656e61b8aa4b638ac9dd8345f2e8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.41376  5.09887]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.clip=0.3\n",
    "learner.fit(0.01, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"kam-test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def proc_str(s): return text_field.preprocess(text_field.tokenize(s))\n",
    "def num_str(s): return text_field.numericalize([proc_str(s)])\n",
    "model=learner.model\n",
    "\n",
    "def sample_model(m, s, l=50):\n",
    "    t = num_str(s)\n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(t)\n",
    "    print('...', end='')    \n",
    "\n",
    "    for i in range(l):\n",
    "        pred = res\n",
    "        # get top 5 predictions\n",
    "#         top_pred = pred.topk(5) #.exp() #/ pred[-1].exp().sum()    \n",
    "        top_prob = 3\n",
    "        rand = randint(0, top_prob - 1)\n",
    "        _, top_indices = pred[-1].topk(top_prob)\n",
    "        # Randomly sample one word\n",
    "        n = top_indices.data[rand]\n",
    "        word = text_field.vocab.itos[n]\n",
    "        print(word, end=' ')\n",
    "        if word=='<eos>': break\n",
    "        if word==',': print(\"\\n\")\n",
    "        res,*_ = m(num_str(word))\n",
    "\n",
    "    m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...of the <unk> and i do it like the same <unk> <unk> - a <unk> - <unk> , \n",
      "\n",
      "i 'm the <unk> , \n",
      "\n",
      "<unk> , \n",
      "\n",
      "<unk> - <unk> , \n",
      "\n",
      "<unk> , \n",
      "\n",
      "<unk> <unk> e <unk> <unk> , \n",
      "\n",
      "e <unk> <unk> e e e e , \n",
      "\n",
      "a a a a , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_model(model, \"One\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b5b1107a044f9da274a19d654fd8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/631 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/631 [00:00<?, ?it/s, loss=8.96]\u001b[A\n",
      "  0%|          | 1/631 [00:00<09:17,  1.13it/s, loss=8.96]\u001b[A\n",
      "  0%|          | 1/631 [00:01<17:28,  1.66s/it, loss=8.49]\u001b[A\n",
      "  0%|          | 2/631 [00:01<08:43,  1.20it/s, loss=8.49]\u001b[A\n",
      "  0%|          | 2/631 [00:02<12:52,  1.23s/it, loss=8.58]\u001b[A\n",
      "  0%|          | 3/631 [00:02<08:34,  1.22it/s, loss=8.58]\u001b[A\n",
      "  0%|          | 3/631 [00:03<11:14,  1.07s/it, loss=8.4] \u001b[A\n",
      "  1%|          | 4/631 [00:03<08:25,  1.24it/s, loss=8.4]\u001b[A\n",
      "  1%|          | 4/631 [00:03<10:21,  1.01it/s, loss=8.15]\u001b[A\n",
      "  1%|          | 5/631 [00:03<08:16,  1.26it/s, loss=8.15]\u001b[A\n",
      "  1%|          | 5/631 [00:04<09:53,  1.05it/s, loss=7.97]\u001b[A\n",
      "  1%|          | 6/631 [00:04<08:14,  1.26it/s, loss=7.97]\u001b[A\n",
      "  1%|          | 6/631 [00:05<09:32,  1.09it/s, loss=7.84]\u001b[A\n",
      "  1%|          | 7/631 [00:05<08:09,  1.27it/s, loss=7.84]\u001b[A\n",
      "  1%|          | 7/631 [00:06<09:17,  1.12it/s, loss=7.74]\u001b[A\n",
      "  1%|▏         | 8/631 [00:06<08:07,  1.28it/s, loss=7.74]\u001b[A\n",
      "  1%|▏         | 8/631 [00:07<09:07,  1.14it/s, loss=7.91]\u001b[A\n",
      "  1%|▏         | 9/631 [00:07<08:05,  1.28it/s, loss=7.91]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/envs/latest-fastai/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/ubuntu/anaconda2/envs/latest-fastai/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 9/631 [00:07<08:56,  1.16it/s, loss=7.84]\u001b[A\n",
      "  2%|▏         | 10/631 [00:07<08:01,  1.29it/s, loss=7.84]\u001b[A\n",
      "  2%|▏         | 10/631 [00:08<08:48,  1.17it/s, loss=7.79]\u001b[A\n",
      "  2%|▏         | 11/631 [00:08<08:00,  1.29it/s, loss=7.79]\u001b[A\n",
      "  2%|▏         | 11/631 [00:09<08:42,  1.19it/s, loss=7.71]\u001b[A\n",
      "  2%|▏         | 12/631 [00:09<07:58,  1.29it/s, loss=7.71]\u001b[A\n",
      "  2%|▏         | 12/631 [00:10<08:38,  1.19it/s, loss=7.66]\u001b[A\n",
      "  2%|▏         | 13/631 [00:10<07:57,  1.29it/s, loss=7.66]\u001b[A\n",
      "  2%|▏         | 13/631 [00:10<08:34,  1.20it/s, loss=7.59]\u001b[A\n",
      "  2%|▏         | 14/631 [00:10<07:57,  1.29it/s, loss=7.59]\u001b[A\n",
      "  2%|▏         | 14/631 [00:11<08:30,  1.21it/s, loss=7.55]\u001b[A\n",
      "  2%|▏         | 15/631 [00:11<07:55,  1.29it/s, loss=7.55]\u001b[A\n",
      "  2%|▏         | 15/631 [00:12<08:27,  1.21it/s, loss=7.55]\u001b[A\n",
      "  3%|▎         | 16/631 [00:12<07:54,  1.30it/s, loss=7.55]\u001b[A\n",
      "  3%|▎         | 16/631 [00:13<08:23,  1.22it/s, loss=7.54]\u001b[A\n",
      "  3%|▎         | 17/631 [00:13<07:53,  1.30it/s, loss=7.54]\u001b[A\n",
      "  3%|▎         | 17/631 [00:13<08:20,  1.23it/s, loss=7.51]\u001b[A\n",
      "  3%|▎         | 18/631 [00:13<07:52,  1.30it/s, loss=7.51]\u001b[A\n",
      "  3%|▎         | 18/631 [00:14<08:17,  1.23it/s, loss=7.47]\u001b[A\n",
      "  3%|▎         | 19/631 [00:14<07:50,  1.30it/s, loss=7.47]\u001b[A\n",
      "  3%|▎         | 19/631 [00:15<08:15,  1.24it/s, loss=7.46]\u001b[A\n",
      "  3%|▎         | 20/631 [00:15<07:49,  1.30it/s, loss=7.46]\u001b[A\n",
      "  3%|▎         | 20/631 [00:16<08:13,  1.24it/s, loss=7.44]\u001b[A\n",
      "  3%|▎         | 21/631 [00:16<07:49,  1.30it/s, loss=7.44]\u001b[A\n",
      "  3%|▎         | 21/631 [00:16<08:11,  1.24it/s, loss=7.42]\u001b[A\n",
      "  3%|▎         | 22/631 [00:16<07:48,  1.30it/s, loss=7.42]\u001b[A\n",
      "  3%|▎         | 22/631 [00:17<08:09,  1.25it/s, loss=7.41]\u001b[A\n",
      "  4%|▎         | 23/631 [00:17<07:47,  1.30it/s, loss=7.41]\u001b[A\n",
      "  4%|▎         | 23/631 [00:18<08:07,  1.25it/s, loss=8.27]\u001b[A\n",
      "  4%|▍         | 24/631 [00:18<07:46,  1.30it/s, loss=8.27]\u001b[A\n",
      "  4%|▍         | 24/631 [00:19<08:05,  1.25it/s, loss=8.85]\u001b[A\n",
      "  4%|▍         | 25/631 [00:19<07:45,  1.30it/s, loss=8.85]\u001b[A\n",
      "  4%|▍         | 25/631 [00:19<08:04,  1.25it/s, loss=9.18]\u001b[A\n",
      "  4%|▍         | 26/631 [00:19<07:44,  1.30it/s, loss=9.18]\u001b[A\n",
      "  4%|▍         | 26/631 [00:20<08:01,  1.26it/s, loss=9.42]\u001b[A\n",
      "  4%|▍         | 27/631 [00:20<07:43,  1.30it/s, loss=9.42]\u001b[A\n",
      "  4%|▍         | 27/631 [00:21<08:00,  1.26it/s, loss=9.66]\u001b[A\n",
      "  4%|▍         | 28/631 [00:21<07:42,  1.30it/s, loss=9.66]\u001b[A\n",
      "  4%|▍         | 28/631 [00:22<07:58,  1.26it/s, loss=9.98]\u001b[A\n",
      "  5%|▍         | 29/631 [00:22<07:41,  1.31it/s, loss=9.98]\u001b[A\n",
      "  5%|▍         | 29/631 [00:22<07:49,  1.28it/s, loss=10.4]\u001b[A\n",
      "  5%|▍         | 30/631 [00:22<07:32,  1.33it/s, loss=10.4]\u001b[A\n",
      "  5%|▍         | 30/631 [00:22<07:40,  1.31it/s, loss=10.9]\u001b[A\n",
      "  5%|▍         | 31/631 [00:22<07:24,  1.35it/s, loss=10.9]\u001b[A\n",
      "  5%|▍         | 31/631 [00:23<07:38,  1.31it/s, loss=11.5]\u001b[A\n",
      "  5%|▌         | 32/631 [00:23<07:23,  1.35it/s, loss=11.5]\u001b[A\n",
      "  5%|▌         | 32/631 [00:24<07:37,  1.31it/s, loss=12.2]\u001b[A\n",
      "  5%|▌         | 33/631 [00:24<07:22,  1.35it/s, loss=12.2]\u001b[A\n",
      "  5%|▌         | 33/631 [00:25<07:36,  1.31it/s, loss=12.8]\u001b[A\n",
      "  5%|▌         | 34/631 [00:25<07:22,  1.35it/s, loss=12.8]\u001b[A\n",
      "  5%|▌         | 34/631 [00:25<07:35,  1.31it/s, loss=13.4]\u001b[A\n",
      "  6%|▌         | 35/631 [00:25<07:21,  1.35it/s, loss=13.4]\u001b[A\n",
      "  6%|▌         | 35/631 [00:26<07:34,  1.31it/s, loss=13.9]\u001b[A\n",
      "  6%|▌         | 36/631 [00:26<07:21,  1.35it/s, loss=13.9]\u001b[A\n",
      "  6%|▌         | 36/631 [00:27<07:33,  1.31it/s, loss=14.3]\u001b[A\n",
      "  6%|▌         | 37/631 [00:27<07:20,  1.35it/s, loss=14.3]\u001b[A\n",
      "  6%|▌         | 37/631 [00:28<07:32,  1.31it/s, loss=14.1]\u001b[A\n",
      "  6%|▌         | 38/631 [00:28<07:20,  1.35it/s, loss=14.1]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cc717da78434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 160\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.fit(0.05, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add93c23b4fd41869ef851d31ec4a5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.16789  4.82935]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.005, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"kam-test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('kam-test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a24f8a0b623462e9e4ca7d16e0cbd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.18342  4.74189]                                \n",
      "[ 1.       5.15557  4.69803]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"kam-test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(\"kam-test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc971daa3144b539ff118ad5f8e0905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.11138  4.68021]                                \n",
      "[ 1.       5.10083  4.67461]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.0001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"kam-test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model2(model, s, num_words=50, random_level=4):\n",
    "    words = s\n",
    "    t = num_str(s)\n",
    "    model[0].bs=1\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    res,*_ = model(t)\n",
    "    print('...', end='')\n",
    "\n",
    "    for i in range(num_words):\n",
    "        pred = res\n",
    "        # get top number of predictions\n",
    "        rand = randint(0, random_level - 1)\n",
    "        _, top_indices = pred[-1].topk(random_level)\n",
    "        # Randomly sample one word\n",
    "        n = top_indices.data[rand]\n",
    "        word = text_field.vocab.itos[n]\n",
    "        print(word, end=' ')\n",
    "        if word=='<eos>': break\n",
    "        if word=='<unk>': continue\n",
    "        if word==',': print(\"\\n\")\n",
    "\n",
    "        words += \" \" + word\n",
    "        num_words_to_remember = 200\n",
    "        if len(words.split(\" \")) > num_words_to_remember:\n",
    "            words = \"\".join(words.split(\" \")[-num_words_to_remember:])\n",
    "        res,*_ = model(num_str(words))\n",
    "\n",
    "    model[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...when it comes and i do nt wanna be the only thing i love i know , \n",
      "\n",
      "i do i do n't need it but it feels good , \n",
      "\n",
      "you 're the only thing you want i love you when it goes i 'm on it i love the world that i love it i do it , \n",
      "\n",
      "oh , \n",
      "\n",
      "i love the time it feels good i 'm so good , \n",
      "\n",
      "i wanna go to the sky and i do n't want no more , \n",
      "\n",
      "you do it , \n",
      "\n",
      "i love you when i 'm in it when you get the money , \n",
      "\n",
      "yeah , \n",
      "\n",
      "yeah you know it i do , \n",
      "\n",
      "it 's all about the good time and it do i need to do it i want the way it 's the only way you got it you know it i 'm the way it 's the one that you want i know it i know , \n",
      "\n",
      "you 're so good i can see that you 're in my way , \n",
      "\n",
      "it ai the way you want to do , \n",
      "\n",
      "you can get your time on my life and if it ai all over and , \n",
      "\n",
      "<unk> "
     ]
    }
   ],
   "source": [
    "sample_model2(model, \"I love you\", num_words=200, random_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..., \n",
      "\n",
      "we all in love , \n",
      "\n",
      "fuck we all we love , \n",
      "\n",
      "yeah yeah yeah i 'm all the bitches yeah yeah we got it on all the bitches i 'm the shit yeah yeah we got ta get the fuck up yeah we be on that , \n",
      "\n",
      "all these bottles we got ta get it , \n",
      "\n",
      "all night yeah yeah , \n",
      "\n",
      "i know that you 're all i need , \n",
      "\n",
      "you 're my girl i 'm the shit i want to make it , \n",
      "\n",
      "yeah yeah yeah i know , \n",
      "\n",
      "all my money i want you , \n",
      "\n",
      "i know you know i 'm on that shit yeah , \n",
      "\n",
      "you do that , \n",
      "\n",
      "all that you got , \n",
      "\n",
      "i got it , \n",
      "\n",
      "you , \n",
      "\n",
      "yeah , \n",
      "\n",
      "yeah you do n't know , \n",
      "\n",
      "i know you know it i 'm a make a dance , \n",
      "\n",
      "oh oh yeah i do what i want i like , \n",
      "\n",
      "you want it i do it for me , \n",
      "\n",
      "yeah i want you i need , \n",
      "\n",
      "i want you , \n",
      "\n",
      "yeah yeah i love my way <eos> "
     ]
    }
   ],
   "source": [
    "sample_model2(model, \"Shit\", num_words=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(\"kam-test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f805fe4b9a741da9eb0565c0eca0cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.10981  4.67423]                                \n",
      "[ 1.       5.11601  4.67587]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.00001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471a96d2090e4eb191389ecf4b4769cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.19002  4.69439]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2ba753e7714d8db58bcccb593135cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.083    4.63809]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-4, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('kam-test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f7a10323b143488f686cae4e69bac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=26), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       5.08655  4.62619]                                \n",
      "[ 1.       5.07559  4.62677]                                \n",
      "[ 2.       5.08182  4.60987]                                \n",
      "[ 3.       5.06883  4.60078]                                \n",
      "[ 4.       5.05049  4.58923]                                \n",
      "[ 5.       5.04977  4.5873 ]                                \n",
      "[ 6.       5.06919  4.58579]                                \n",
      "[ 7.       5.04963  4.58734]                                \n",
      "[ 8.       5.04825  4.58122]                                \n",
      "[ 9.       5.04068  4.56802]                                \n",
      "[ 10.        5.0298    4.56243]                             \n",
      "[ 11.        5.01615   4.55334]                             \n",
      "[ 12.        5.00283   4.54655]                             \n",
      "[ 13.        4.99498   4.54109]                             \n",
      "[ 14.        4.98789   4.53393]                             \n",
      "[ 15.        4.99476   4.52855]                             \n",
      "[ 16.        4.98982   4.5252 ]                             \n",
      "[ 17.        4.98826   4.52058]                             \n",
      "[ 18.        4.96797   4.51455]                             \n",
      "[ 19.        4.96112   4.51481]                             \n",
      " 85%|████████▌ | 539/631 [06:39<01:08,  1.35it/s, loss=4.97]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9ba648fd6056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 160\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.fit(3e-4, 3, wds=1e-6, cycle_len=2, cycle_mult=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('kam-test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368dd365dc6e41eda372d3cf69273fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/631 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/631 [00:00<?, ?it/s, loss=5.13]\u001b[A\n",
      "  0%|          | 1/631 [00:00<09:10,  1.15it/s, loss=5.13]\u001b[A\n",
      "  0%|          | 1/631 [00:01<16:42,  1.59s/it, loss=5.09]\u001b[A\n",
      "  0%|          | 2/631 [00:01<08:20,  1.26it/s, loss=5.09]\u001b[A\n",
      "  0%|          | 2/631 [00:02<12:20,  1.18s/it, loss=5.04]\u001b[A\n",
      "  0%|          | 3/631 [00:02<08:13,  1.27it/s, loss=5.04]\u001b[A\n",
      "  0%|          | 3/631 [00:03<10:48,  1.03s/it, loss=5.07]\u001b[A\n",
      "  1%|          | 4/631 [00:03<08:05,  1.29it/s, loss=5.07]\u001b[A\n",
      "  1%|          | 4/631 [00:03<10:04,  1.04it/s, loss=5.06]\u001b[A\n",
      "  1%|          | 5/631 [00:03<08:03,  1.30it/s, loss=5.06]\u001b[A\n",
      "  1%|          | 5/631 [00:04<09:37,  1.08it/s, loss=5.06]\u001b[A\n",
      "  1%|          | 6/631 [00:04<08:00,  1.30it/s, loss=5.06]\u001b[A\n",
      "  1%|          | 6/631 [00:05<09:20,  1.12it/s, loss=5.04]\u001b[A\n",
      "  1%|          | 7/631 [00:05<07:59,  1.30it/s, loss=5.04]\u001b[A\n",
      "  1%|          | 7/631 [00:06<09:05,  1.14it/s, loss=5.03]\u001b[A\n",
      "  1%|▏         | 8/631 [00:06<07:56,  1.31it/s, loss=5.03]\u001b[A\n",
      "  1%|▏         | 8/631 [00:06<08:24,  1.23it/s, loss=5.03]\u001b[A\n",
      "  1%|▏         | 9/631 [00:06<07:28,  1.39it/s, loss=5.03]\u001b[A\n",
      "  1%|▏         | 9/631 [00:06<07:55,  1.31it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 10/631 [00:06<07:06,  1.45it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 10/631 [00:07<07:54,  1.31it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 11/631 [00:07<07:11,  1.44it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 11/631 [00:08<07:53,  1.31it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 12/631 [00:08<07:13,  1.43it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 12/631 [00:09<07:52,  1.31it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 13/631 [00:09<07:15,  1.42it/s, loss=5.04]\u001b[A\n",
      "  2%|▏         | 13/631 [00:09<07:50,  1.31it/s, loss=5.02]\u001b[A\n",
      "  2%|▏         | 14/631 [00:09<07:16,  1.41it/s, loss=5.02]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/envs/latest-fastai/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/ubuntu/anaconda2/envs/latest-fastai/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 14/631 [00:10<07:48,  1.32it/s, loss=5.01]\u001b[A\n",
      "  2%|▏         | 15/631 [00:10<07:17,  1.41it/s, loss=5.01]\u001b[A\n",
      "  2%|▏         | 15/631 [00:11<07:47,  1.32it/s, loss=4.99]\u001b[A\n",
      "  3%|▎         | 16/631 [00:11<07:17,  1.40it/s, loss=4.99]\u001b[A\n",
      "  3%|▎         | 16/631 [00:12<07:46,  1.32it/s, loss=4.98]\u001b[A\n",
      "  3%|▎         | 17/631 [00:12<07:18,  1.40it/s, loss=4.98]\u001b[A\n",
      "  3%|▎         | 17/631 [00:12<07:46,  1.32it/s, loss=4.98]\u001b[A\n",
      "  3%|▎         | 18/631 [00:12<07:19,  1.39it/s, loss=4.98]\u001b[A\n",
      "  3%|▎         | 18/631 [00:13<07:45,  1.32it/s, loss=4.97]\u001b[A\n",
      "  3%|▎         | 19/631 [00:13<07:20,  1.39it/s, loss=4.97]\u001b[A\n",
      "  3%|▎         | 19/631 [00:14<07:45,  1.32it/s, loss=4.97]\u001b[A\n",
      "  3%|▎         | 20/631 [00:14<07:21,  1.38it/s, loss=4.97]\u001b[A\n",
      "  3%|▎         | 20/631 [00:15<07:44,  1.31it/s, loss=4.98]\u001b[A\n",
      "  3%|▎         | 21/631 [00:15<07:22,  1.38it/s, loss=4.98]\u001b[A\n",
      "  3%|▎         | 21/631 [00:15<07:44,  1.31it/s, loss=4.99]\u001b[A\n",
      "  3%|▎         | 22/631 [00:16<07:22,  1.37it/s, loss=4.99]\u001b[A\n",
      "  3%|▎         | 22/631 [00:16<07:33,  1.34it/s, loss=4.99]\u001b[A\n",
      "  4%|▎         | 23/631 [00:16<07:12,  1.40it/s, loss=4.99]\u001b[A\n",
      "  4%|▎         | 23/631 [00:17<07:32,  1.35it/s, loss=5]   \u001b[A\n",
      "  4%|▍         | 24/631 [00:17<07:12,  1.40it/s, loss=5]\u001b[A\n",
      "  4%|▍         | 24/631 [00:17<07:31,  1.34it/s, loss=5]\u001b[A\n",
      "  4%|▍         | 25/631 [00:17<07:13,  1.40it/s, loss=5]\u001b[A\n",
      "  4%|▍         | 25/631 [00:18<07:31,  1.34it/s, loss=5.01]\u001b[A\n",
      "  4%|▍         | 26/631 [00:18<07:13,  1.40it/s, loss=5.01]\u001b[A\n",
      "  4%|▍         | 26/631 [00:19<07:30,  1.34it/s, loss=5.01]\u001b[A\n",
      "  4%|▍         | 27/631 [00:19<07:13,  1.39it/s, loss=5.01]\u001b[A\n",
      "  4%|▍         | 27/631 [00:20<07:30,  1.34it/s, loss=5]   \u001b[A\n",
      "  4%|▍         | 28/631 [00:20<07:13,  1.39it/s, loss=5]\u001b[A\n",
      "  4%|▍         | 28/631 [00:20<07:30,  1.34it/s, loss=5]\u001b[A\n",
      "  5%|▍         | 29/631 [00:20<07:14,  1.39it/s, loss=5]\u001b[A\n",
      "  5%|▍         | 29/631 [00:21<07:30,  1.34it/s, loss=4.99]\u001b[A\n",
      "  5%|▍         | 30/631 [00:21<07:14,  1.38it/s, loss=4.99]\u001b[A\n",
      "  5%|▍         | 30/631 [00:22<07:29,  1.34it/s, loss=5]   \u001b[A\n",
      "  5%|▍         | 31/631 [00:22<07:14,  1.38it/s, loss=5]\u001b[A\n",
      "  5%|▍         | 31/631 [00:23<07:29,  1.34it/s, loss=5]\u001b[A\n",
      "  5%|▌         | 32/631 [00:23<07:14,  1.38it/s, loss=5]\u001b[A\n",
      "  5%|▌         | 32/631 [00:23<07:28,  1.33it/s, loss=5]\u001b[A\n",
      "  5%|▌         | 33/631 [00:23<07:14,  1.38it/s, loss=5]\u001b[A\n",
      "  5%|▌         | 33/631 [00:24<07:28,  1.33it/s, loss=5.02]\u001b[A\n",
      "  5%|▌         | 34/631 [00:24<07:14,  1.37it/s, loss=5.02]\u001b[A\n",
      "  5%|▌         | 34/631 [00:25<07:28,  1.33it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 35/631 [00:25<07:14,  1.37it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 35/631 [00:26<07:27,  1.33it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 36/631 [00:26<07:14,  1.37it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 36/631 [00:26<07:21,  1.35it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 37/631 [00:26<07:08,  1.39it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 37/631 [00:27<07:20,  1.35it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 38/631 [00:27<07:08,  1.38it/s, loss=5.02]\u001b[A\n",
      "  6%|▌         | 38/631 [00:28<07:20,  1.35it/s, loss=5.03]\u001b[A\n",
      "  6%|▌         | 39/631 [00:28<07:08,  1.38it/s, loss=5.03]\u001b[A\n",
      "  6%|▌         | 39/631 [00:28<07:19,  1.35it/s, loss=5.03]\u001b[A\n",
      "  6%|▋         | 40/631 [00:28<07:08,  1.38it/s, loss=5.03]\u001b[A\n",
      "  6%|▋         | 40/631 [00:29<07:19,  1.35it/s, loss=5.03]\u001b[A\n",
      "  6%|▋         | 41/631 [00:29<07:07,  1.38it/s, loss=5.03]\u001b[A\n",
      "  6%|▋         | 41/631 [00:30<07:18,  1.34it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 42/631 [00:30<07:07,  1.38it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 42/631 [00:31<07:18,  1.34it/s, loss=5.02]\u001b[A\n",
      "  7%|▋         | 43/631 [00:31<07:07,  1.37it/s, loss=5.02]\u001b[A\n",
      "  7%|▋         | 43/631 [00:32<07:18,  1.34it/s, loss=5.02]\u001b[A\n",
      "  7%|▋         | 44/631 [00:32<07:07,  1.37it/s, loss=5.02]\u001b[A\n",
      "  7%|▋         | 44/631 [00:32<07:17,  1.34it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 45/631 [00:32<07:07,  1.37it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 45/631 [00:33<07:17,  1.34it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 46/631 [00:33<07:07,  1.37it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 46/631 [00:34<07:16,  1.34it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 47/631 [00:34<07:06,  1.37it/s, loss=5.03]\u001b[A\n",
      "  7%|▋         | 47/631 [00:35<07:16,  1.34it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 48/631 [00:35<07:06,  1.37it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 48/631 [00:35<07:15,  1.34it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 49/631 [00:35<07:06,  1.37it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 49/631 [00:36<07:15,  1.34it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 50/631 [00:36<07:05,  1.36it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 50/631 [00:37<07:14,  1.34it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 51/631 [00:37<07:05,  1.36it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 51/631 [00:38<07:13,  1.34it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 52/631 [00:38<07:04,  1.36it/s, loss=5.03]\u001b[A\n",
      "  8%|▊         | 52/631 [00:38<07:13,  1.34it/s, loss=5.04]\u001b[A\n",
      "  8%|▊         | 53/631 [00:38<07:04,  1.36it/s, loss=5.04]\u001b[A\n",
      "  8%|▊         | 53/631 [00:39<07:12,  1.34it/s, loss=5.04]\u001b[A\n",
      "  9%|▊         | 54/631 [00:39<07:03,  1.36it/s, loss=5.04]\u001b[A\n",
      "  9%|▊         | 54/631 [00:40<07:11,  1.34it/s, loss=5.04]\u001b[A\n",
      "  9%|▊         | 55/631 [00:40<07:02,  1.36it/s, loss=5.04]\u001b[A\n",
      "  9%|▊         | 55/631 [00:41<07:10,  1.34it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 56/631 [00:41<07:01,  1.36it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 56/631 [00:41<07:09,  1.34it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 57/631 [00:41<07:01,  1.36it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 57/631 [00:42<07:09,  1.34it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 58/631 [00:42<07:01,  1.36it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 58/631 [00:43<07:08,  1.34it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 59/631 [00:43<07:00,  1.36it/s, loss=5.04]\u001b[A\n",
      "  9%|▉         | 59/631 [00:44<07:07,  1.34it/s, loss=5.05]\u001b[A\n",
      " 10%|▉         | 60/631 [00:44<07:00,  1.36it/s, loss=5.05]\u001b[A\n",
      " 10%|▉         | 60/631 [00:44<07:07,  1.34it/s, loss=5.04]\u001b[A\n",
      " 10%|▉         | 61/631 [00:44<06:59,  1.36it/s, loss=5.04]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-08706551a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 160\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/new_fastai/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/latest-fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...and i got it for the money , \n",
      "\n",
      "i do for my money , \n",
      "\n",
      "yeah i do for it all my bitches , \n",
      "\n",
      "all the money i love 'em , \n",
      "\n",
      "yeah , \n",
      "\n",
      "fuck all my hoes , \n",
      "\n",
      "all these hoes bitches love 'em fuck niggas fuck me bitches i love "
     ]
    }
   ],
   "source": [
    "sample_model2(learner.model, \"take it back to the crib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
